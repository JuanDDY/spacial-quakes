{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from obspy import read, UTCDateTime\n",
    "from scipy.signal import spectrogram\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "catalog = pd.read_csv(\"complete_catalog.csv\")\n",
    "stream = read(\"complete_stream.mseed\")\n",
    "\n",
    "def apply_lowpass_filter(signal):\n",
    "    st_low_pass = signal.copy()\n",
    "    st_low_pass.filter(\"lowpass\", freq=1)  \n",
    "    return st_low_pass\n",
    "\n",
    "def signal_to_spectrogram(signal, sampling_rate):\n",
    "    f, t, Sxx = spectrogram(signal, fs=sampling_rate)\n",
    "    return Sxx\n",
    "\n",
    "\n",
    "def get_class_weights(y):\n",
    "    total = len(y)\n",
    "    class_0_weight = (1 / np.sum(y == 0)) * (total / 2.0)\n",
    "    class_1_weight = (1 / np.sum(y == 1)) * (total / 2.0)\n",
    "    return {0: class_0_weight, 1: class_1_weight}\n",
    "\n",
    "sample_size = 1022\n",
    "\n",
    "\n",
    "def load_and_process_data(catalog, stream, max_len=4000, sample_size=sample_size):\n",
    "    X = []\n",
    "    y = []\n",
    "    \n",
    "    \n",
    "    catalog_sample = catalog.sample(n=sample_size, random_state=42)\n",
    "    \n",
    "    for index, row in catalog_sample.iterrows():\n",
    "        start_time = UTCDateTime(row['starttime'])\n",
    "        end_time = UTCDateTime(row['endtime'])\n",
    "        label = row['class']  \n",
    "\n",
    "        \n",
    "        signal = stream.slice(starttime=start_time, endtime=end_time)\n",
    "        if len(signal) == 0:\n",
    "            continue\n",
    "        signal_filtered = apply_lowpass_filter(signal)\n",
    "        signal_data = np.hstack([tr.data for tr in signal_filtered])\n",
    "\n",
    "        \n",
    "        sampling_rate = signal[0].stats.sampling_rate\n",
    "\n",
    "        \n",
    "        if len(signal_data) > max_len:\n",
    "            signal_data = signal_data[:max_len]\n",
    "        elif len(signal_data) < max_len:\n",
    "            signal_data = np.pad(signal_data, (0, max_len - len(signal_data)), 'constant')\n",
    "\n",
    "        \n",
    "        Sxx = signal_to_spectrogram(signal_data, sampling_rate)\n",
    "        X.append(Sxx)\n",
    "        y.append(label)\n",
    "\n",
    "    X = np.array(X)\n",
    "    y = np.array(y)\n",
    "    \n",
    "    if len(X.shape) == 3:  \n",
    "        X = np.expand_dims(X, axis=-1)  \n",
    "    \n",
    "    return X, y\n",
    "\n",
    "X, y = load_and_process_data(catalog, stream, sample_size=sample_size)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "\n",
    "X_flat = X.reshape(X.shape[0], -1)\n",
    "\n",
    "#\n",
    "X_scaled = scaler.fit_transform(X_flat)\n",
    "\n",
    "\n",
    "X_scaled = X_scaled.reshape(X.shape)\n",
    "\n",
    "\n",
    "y_cat = to_categorical(y)\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y_cat, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "class_weights = get_class_weights(y)\n",
    "\n",
    "\n",
    "def build_unet_autoencoder(input_shape):\n",
    "    inputs = layers.Input(shape=input_shape)\n",
    "\n",
    "    c1 = layers.Conv2D(32, (3, 3), activation='relu', padding='same')(inputs)\n",
    "    p1 = layers.MaxPooling2D((2, 2), padding='same')(c1)\n",
    "\n",
    "    c2 = layers.Conv2D(64, (3, 3), activation='relu', padding='same')(p1)\n",
    "    p2 = layers.MaxPooling2D((2, 2), padding='same')(c2)\n",
    "\n",
    "    c3 = layers.Conv2D(128, (3, 3), activation='relu', padding='same')(p2)\n",
    "    \n",
    "\n",
    "    u1 = layers.UpSampling2D((2, 2))(c3)\n",
    "    c4 = layers.Conv2D(64, (3, 3), activation='relu', padding='same')(u1)\n",
    "\n",
    "    u2 = layers.UpSampling2D((2, 2))(c4)\n",
    "    c5 = layers.Conv2D(32, (3, 3), activation='relu', padding='same')(u2)\n",
    "\n",
    "    outputs = layers.Conv2D(1, (1, 1), activation='sigmoid', padding='same')(c5)\n",
    "\n",
    "\n",
    "    output_shape = outputs.shape[1:3]\n",
    "    input_shape = inputs.shape[1:3]\n",
    "    if output_shape != input_shape:\n",
    "        crop_height = output_shape[0] - input_shape[0]\n",
    "        crop_width = output_shape[1] - input_shape[1]\n",
    "        if crop_height > 0 or crop_width > 0:\n",
    "            outputs = layers.Cropping2D(((0, crop_height), (0, crop_width)))(outputs)\n",
    "\n",
    "    autoencoder = models.Model(inputs, outputs)\n",
    "    return autoencoder\n",
    "\n",
    "\n",
    "input_shape = (X_train.shape[1], X_train.shape[2], X_train.shape[3])\n",
    "autoencoder = build_unet_autoencoder(input_shape)\n",
    "optimizer = Adam(learning_rate=0.001)\n",
    "autoencoder.compile(optimizer=optimizer, loss='mse')\n",
    "\n",
    "\n",
    "history = autoencoder.fit(X_train, X_train, epochs=3, batch_size=16, validation_data=(X_test, X_test))\n",
    "\n",
    "decoded_signals = autoencoder.predict(X_test)\n",
    "\n",
    "print(\"Evaluación del autoencoder en datos de prueba:\")\n",
    "autoencoder.evaluate(X_test, X_test)\n",
    "\n",
    "def build_classifier(input_shape):\n",
    "    model = models.Sequential()\n",
    "    model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=input_shape, padding='same'))\n",
    "    model.add(layers.MaxPooling2D((2, 2), padding='same'))\n",
    "    model.add(layers.Conv2D(64, (3, 3), activation='relu', padding='same'))\n",
    "    model.add(layers.MaxPooling2D((2, 2), padding='same'))\n",
    "    model.add(layers.Flatten())\n",
    "    model.add(layers.Dense(64, activation='relu'))\n",
    "    model.add(layers.Dense(2, activation='sigmoid')) \n",
    "    return model\n",
    "\n",
    "classifier_optimizer = Adam(learning_rate=0.001)\n",
    "\n",
    "classifier = build_classifier(input_shape)\n",
    "classifier.compile(optimizer=classifier_optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "classifier.fit(X_train, y_train, epochs=100, batch_size=16, validation_data=(X_test, y_test), class_weight=class_weights)\n",
    "\n",
    "\n",
    "print(\"Evaluación del clasificador en datos de prueba:\")\n",
    "classifier.evaluate(X_test, y_test)\n",
    "\n",
    "\n",
    "y_pred = np.argmax(classifier.predict(X_test), axis=1)\n",
    "y_true = np.argmax(y_test, axis=1)\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "report = classification_report(y_true, y_pred)\n",
    "\n",
    "print(\"Matriz de confusión:\\n\", cm)\n",
    "print(\"Reporte de clasificación:\\n\", report)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix, precision_score, recall_score, f1_score\n",
    "\n",
    "\n",
    "y_pred_test = classifier.predict(X_test)\n",
    "y_pred_train = classifier.predict(X_train)\n",
    "\n",
    "y_pred_classes_test = np.argmax(y_pred_test, axis=1)\n",
    "y_true_classes_test = np.argmax(y_test, axis=1)\n",
    "\n",
    "y_pred_classes_train = np.argmax(y_pred_train, axis=1)\n",
    "y_true_classes_train = np.argmax(y_train, axis=1)\n",
    "\n",
    "precision_test = precision_score(y_true_classes_test, y_pred_classes_test, average=None)\n",
    "recall_test = recall_score(y_true_classes_test, y_pred_classes_test, average=None)\n",
    "f1_test = f1_score(y_true_classes_test, y_pred_classes_test, average=None)\n",
    "\n",
    "\n",
    "precision_train = precision_score(y_true_classes_train, y_pred_classes_train, average=None)\n",
    "recall_train = recall_score(y_true_classes_train, y_pred_classes_train, average=None)\n",
    "f1_train = f1_score(y_true_classes_train, y_pred_classes_train, average=None)\n",
    "\n",
    "\n",
    "report_test = classification_report(y_true_classes_test, y_pred_classes_test, target_names=['No Sismo', 'Sismo'])\n",
    "conf_matrix_test = confusion_matrix(y_true_classes_test, y_pred_classes_test)\n",
    "\n",
    "\n",
    "report_train = classification_report(y_true_classes_train, y_pred_classes_train, target_names=['No Sismo', 'Sismo'])\n",
    "conf_matrix_train = confusion_matrix(y_true_classes_train, y_pred_classes_train)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\"\\nMétricas para el conjunto de ENTRENAMIENTO:\\n\")\n",
    "print(\"Reporte de Clasificación:\\n\", report_train)\n",
    "print(\"Matriz de Confusión:\\n\", conf_matrix_train)\n",
    "\n",
    "\n",
    "print(\"Métricas para el conjunto de PRUEBA:\\n\")\n",
    "print(\"Reporte de Clasificación:\\n\", report_test)\n",
    "print(\"Matriz de Confusión:\\n\", conf_matrix_test)\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
